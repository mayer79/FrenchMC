---
title: "DALEX with French Motor TPL insurance data"
output:
  html_document:
    df_print: paged
---

# Introduction

This notebook uses the bundle `DALEX` (descriptive machine learning explanations) to shed light on black box models in the context of claims frequency models. Some troubles with case weights (= exposure weights) became visible. If okay I will send over this notebook to DALEX developer. 

# Preparation

We start by loading

- libraries,

- functions as well as

- data

```{r, message=FALSE, warning=FALSE}
# LIBRARIES

library(CASdatasets)  # Download zip from http://cas.uqam.ca/ and install from local package source
library(gbm)
library(xgboost)
library(DALEX) # For model interpretation
library(ceterisParibus)
library(tidyverse) # For data prep
library(caret) # For data split

# FUNCTIONS
deviance_poisson <- function(y, pred) {
  2 * mean(pred - y + log((y / pred)^y))
}

deviance_gamma <- function(y, pred) {
  2 * mean((y - pred) / pred - log(y / pred))
}

# Transformer for XGBoost
prep_mat <- function(data, response = TRUE) {
  data[["Power"]] <- match(data[["Power"]], letters)
  data[["Gas"]] <- data[["Gas"]] == "Diesel"
  data.matrix(data[c(if (response) y_name, x_name, w_name)])
}

# DATA PREPARATION

# Load the data
data(freMTPLfreq) 

# Some column names
x_name <- c("CarAge", "DriverAge", "Power", "Gas", "Density")
y_name <- "ClaimNb"
w_name <- "Exposure"

# Train/test split
set.seed(3928272)
ind <- caret::createDataPartition(freMTPLfreq[[y_name]], p = 0.80, list = FALSE) %>% c

trainDF <- freMTPLfreq[ind, ]
validDF <- freMTPLfreq[-ind, ]

trainMat <- prep_mat(freMTPLfreq[ind, ])
validMat <- prep_mat(freMTPLfreq[-ind, ])

trainXgb <- xgb.DMatrix(trainMat[, x_name], 
                        label = trainMat[, y_name] / trainMat[, w_name], 
                        weight = trainMat[, w_name])

```

# The models

We fit three count regressions:

- Poisson GLM: counts as response, log(exposure) as offset

- Poisson tree booster with `gbm`: count es response, log(exposure) as offset

- Poisson tree booster with `xgboost`: count / exposure as response, exposure as case weight

```{r}
freMTPLfreq[1:6, c(y_name, x_name, w_name)] %>% 
  kableExtra::kable()
summary(freMTPLfreq[c(y_name, x_name, w_name)])

form <- ClaimNb ~ offset(log(Exposure)) + CarAge + DriverAge + Power + Gas + Density

# Linear model
(fit_glm <- glm(form, 
                data = trainDF, 
                family = poisson(link = log)))

# Tree booster: gbm
(fit_gbm <- gbm(form, 
                data = trainDF, 
                distribution = "poisson",
                n.trees = 200, 
                interaction.depth = 3, 
                shrinkage = 0.05))

# Tree booster: XGBoost
param_xgb <- list(max_depth = 5, 
                  learning_rate = 0.05, 
                  nthread = 4, 
                  objective = "count:poisson")

(fit_xgb <- xgb.train(param_xgb,
                      data = trainXgb,
                      nrounds = 500))

```

# Explaining the models

We are almost ready to start interpreting the fitted models. First, we need to collect all infos per model required to make predictions and put them into an object called "explainer". It contains

- The fitted model

- A data set to be used to evaluate predictions

- The response for above observations

- A predict function

## Initializing the "explainer"
```{r}
explainer_glm <- explain(fit_glm, 
                         data = validDF[c(x_name, w_name)], 
                         y = validDF[[y_name]], 
                         label = "glm", 
                         predict_function = function(model, x) predict(model, x, type = "response"))

explainer_gbm <- explain(fit_gbm, 
                         data = validDF[c(x_name, w_name)], 
                         y = validDF[[y_name]], 
                         label = "gbm",
                         predict_function = function(model, x) suppressWarnings(predict(model, 
                                              x, n.trees = 200, type = "response") * x[[w_name]]))

explainer_xgb <- explain(fit_xgb, 
                         data = validDF[c(x_name, w_name)], 
                         y = validDF[[y_name]], 
                         label = "xgb",
                         predict_function = function(model, x) {x <- prep_mat(x, FALSE); predict(model, x[, x_name, drop = FALSE]) * x[, w_name]})

explainers <- list(explainer_glm, explainer_gbm, explainer_xgb)
```

## Model performance

A simple way to visualize the results of a model is to look at distributions of residuals and to compare them across models.

```{r}
mp <- lapply(explainers, model_performance)
do.call(plot, c(mp, geom = "boxplot"))
```

*Issue:* The function `model_performance` allows to pass a loss function of the residuals (as single argument) only. In our case, we would like to show deviance residuals which would require a more general interface. We would like to be able to pass a loss function with two arguments: Observed and predicted values. Furthermore, more control over `ggplot` (e.g. coordinate system) would be nice.

## Variable importance on permuting validation data

A model agnostic way to study variable importance is to study the worsening of the total loss if the values in variable X are randomly permuted. An unimportant variable would lead to a small increase in loss if permuted, while an important one would lead to completely wrong predictions and, consequently to a large gain in loss.

```{r}
vd <- lapply(explainers, variable_importance, loss_function = deviance_poisson, n_sample = -1)
vd <- lapply(vd, function(x) subset(x, variable != "Exposure"))
do.call(plot, vd)
```

*Issue:* The variable `Exposure` is required for prediction, but here, we would not want to permute it. We can manually kick that variable out before plotting.

## Variable importance ("prediction breakdown") for one single observation
The permutation approach provides a "global" variable importance. Additionally, it might be interesting to see which variable is responsible by how much for the prediction of an individual observation. For a linear model, this is just the value of the (centered) regressor times the model coefficient. There are different algorithms to do it in a model agnostic way: LIME, LIVE, Shapely and also prediction breakdown. Here, we use the latter. It works by iteratively evaluating how much the average prediction on the evaluation data changes if all values of variable $X$ are replaced by the corresponding value of the one observation of interest. 

```{r, fig.height=10}
sp <- lapply(explainers, single_prediction, observation = validDF[1, ])
do.call(plot, sp)
```

*Issue:* `Exposure` should not appear here. We would like to manually drop certain fields from these calculations. Furthermore, more control over ggplot would be nice (e.g. x axis limits).

## Effect of driver age: ceteris paribus ("everything else being fixed") profiles across models

Here, we pick one or more individual observations and check how their predictions change if the value of a selected variable is systematically being changed. For an additive linear model, the movement is identical for all observations. These plots make sense as long as ceteris paribus interpretations make sense (i.e. if no strong causal relationships to other regressors exist.)

```{r}
picks <- 1
cp <- lapply(explainers, 
             ceteris_paribus, 
             observations = validDF[picks, c(x_name, w_name)])

do.call(plot, c(cp, color = "_label_", selected_variables = "DriverAge"))
```

## Effect of driver age: multiple ceteris paribus profiles for the GLM
```{r}
picks <- 1:20
cp <- ceteris_paribus(explainer_glm, observations = validDF[picks, c(x_name, w_name)])
plot(cp, selected_variables = "DriverAge")
```

## Effect of driver age: multiple ceteris paribus profiles for the XGBoost model
```{r}
cp <- ceteris_paribus(explainer_xgb, observations = validDF[picks, c(x_name, w_name)])
plot(cp, selected_variables = "DriverAge")
```

## Effect of driver age: average of above profiles aka "partial dependence plots"
If multiple ceteris paribus profiles are being aggregated, we end up with Friedman's famous partial dependence plots. 
```{r}
plot(cp, selected_variables = "DriverAge", aggregate_profiles = mean)
```

## Variant: Look at profiles of close neighbours
```{r}
validDF[1, "DriverAge"]
neigh <- select_neighbours(validDF, validDF[1, ])
cp <- ceteris_paribus(explainer_xgb, observations = neigh)
plot(cp, selected_variables = "DriverAge")
```